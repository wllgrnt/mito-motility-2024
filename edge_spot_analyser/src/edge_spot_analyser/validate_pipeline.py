"""
Validation script to compare Python pipeline outputs with CellProfiler outputs.

This script compares the CSV files generated by the Python pipeline against
the example CellProfiler outputs to ensure numerical agreement.

Usage:
    python validate_pipeline.py --cp_output example_CP_output/24111_siRNA/ --py_output validation_output/
"""

import argparse
from pathlib import Path

import numpy as np
import pandas as pd


def compare_dataframes(
    cp_df: pd.DataFrame,
    py_df: pd.DataFrame,
    name: str,
    tolerance: float = 0.05
) -> dict:
    """
    Compare two DataFrames for numerical agreement.

    Args:
        cp_df: CellProfiler output DataFrame
        py_df: Python pipeline output DataFrame
        name: Name of the comparison (for logging)
        tolerance: Relative tolerance for numerical comparison (default 5%)

    Returns:
        Dictionary with comparison results
    """
    results = {
        'name': name,
        'cp_rows': len(cp_df),
        'py_rows': len(py_df),
        'row_count_match': len(cp_df) == len(py_df),
        'discrepancies': []
    }

    # Check row counts
    if not results['row_count_match']:
        results['discrepancies'].append(
            f"Row count mismatch: CP={len(cp_df)}, Python={len(py_df)}"
        )
        return results

    # Find common columns (numerical only)
    common_cols = set(cp_df.columns) & set(py_df.columns)
    numerical_cols = [
        col for col in common_cols
        if pd.api.types.is_numeric_dtype(cp_df[col]) and
           pd.api.types.is_numeric_dtype(py_df[col])
    ]

    results['columns_compared'] = len(numerical_cols)

    # Compare each numerical column
    for col in numerical_cols:
        cp_vals = cp_df[col].values
        py_vals = py_df[col].values

        # Skip if all NaN
        if np.all(np.isnan(cp_vals)) and np.all(np.isnan(py_vals)):
            continue

        # Calculate relative difference
        with np.errstate(divide='ignore', invalid='ignore'):
            rel_diff = np.abs((py_vals - cp_vals) / (np.abs(cp_vals) + 1e-10))

        # Find values exceeding tolerance
        exceeds_tolerance = rel_diff > tolerance

        if np.any(exceeds_tolerance):
            n_exceeds = np.sum(exceeds_tolerance)
            max_diff = np.max(rel_diff[exceeds_tolerance])
            results['discrepancies'].append(
                f"{col}: {n_exceeds}/{len(cp_vals)} values exceed {tolerance*100}% tolerance "
                f"(max diff: {max_diff*100:.2f}%)"
            )

    return results


def validate_pipeline(cp_output_dir: Path, py_output_dir: Path, tolerance: float = 0.05):
    """
    Validate Python pipeline outputs against CellProfiler outputs.

    Args:
        cp_output_dir: Directory containing CellProfiler CSV outputs
        py_output_dir: Directory containing Python pipeline CSV outputs
        tolerance: Relative tolerance for numerical comparison
    """
    print("Comparing outputs:")
    print(f"  CellProfiler: {cp_output_dir}")
    print(f"  Python:       {py_output_dir}")
    print(f"  Tolerance:    {tolerance*100}%\n")

    # Files to compare
    files = ['Nuclei.csv', 'Expand_Nuclei.csv', 'Perinuclear_region.csv', 'Image.csv']

    all_results = []

    for filename in files:
        cp_file = cp_output_dir / filename
        py_file = py_output_dir / filename

        if not cp_file.exists():
            print(f"⚠️  {filename}: CellProfiler file not found, skipping")
            continue

        if not py_file.exists():
            print(f"❌ {filename}: Python output not found")
            continue

        # Load DataFrames
        cp_df = pd.read_csv(cp_file)
        py_df = pd.read_csv(py_file)

        # Compare
        results = compare_dataframes(cp_df, py_df, filename, tolerance)
        all_results.append(results)

        # Print results
        if results['discrepancies']:
            print(f"❌ {filename}:")
            print(f"   Rows: CP={results['cp_rows']}, Python={results['py_rows']}")
            print(f"   Columns compared: {results.get('columns_compared', 0)}")
            for disc in results['discrepancies']:
                print(f"   - {disc}")
        else:
            print(f"✅ {filename}: All measurements agree within {tolerance*100}% tolerance")
            print(f"   Rows: {results['cp_rows']}, Columns compared: {results.get('columns_compared', 0)}")

        print()

    # Summary
    n_passed = sum(1 for r in all_results if not r['discrepancies'])
    n_total = len(all_results)

    print("=" * 60)
    print(f"Summary: {n_passed}/{n_total} files passed validation")
    print("=" * 60)

    if n_passed == n_total:
        print("✅ All files validated successfully!")
        return 0
    else:
        print("❌ Some files have discrepancies")
        return 1


def main():
    parser = argparse.ArgumentParser(
        description='Validate Python pipeline outputs against CellProfiler'
    )

    parser.add_argument(
        '--cp_output',
        type=Path,
        required=True,
        help='Directory with CellProfiler CSV outputs'
    )

    parser.add_argument(
        '--py_output',
        type=Path,
        required=True,
        help='Directory with Python pipeline CSV outputs'
    )

    parser.add_argument(
        '--tolerance',
        type=float,
        default=0.05,
        help='Relative tolerance for numerical comparison (default 0.05 = 5%%)'
    )

    args = parser.parse_args()

    exit_code = validate_pipeline(args.cp_output, args.py_output, args.tolerance)
    exit(exit_code)


if __name__ == '__main__':
    main()
